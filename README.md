**Basic Experiments**

This branch is dedicated to foundational experiments in AI Quality Engineering. It contains small, focused projects that demonstrate my learning process, coding practices, and approach to validating AI/ML systems. The purpose of this branch is to showcase practical exercises that build the groundwork for more advanced work in RAG pipelines and Agent-based evaluations.
  
**Objectives**

  - Develop hands-on familiarity with Git, GitHub, and project workflows.
  - Practice basic coding and testing techniques relevant to AI/ML projects.
  - Create structured, reusable templates for experimentation.
  - Build a foundation that will align with advanced work in other branches (feature/rag-llm and feature/agents).

**Contents**

  The experiments in this branch will cover areas such as: 
  
      1.	Python Basics for AI/ML Testing
            -- Data loading, cleaning, and validation.
            -- Unit testing for small Python functions.
      2.	Evaluation Utilities
            -- Scripts to log experiment results.
            -- Tracking performance metrics (accuracy, precision, recall).
      3.	Version Control Practice
            -- Using Git effectively for small changes.
            -- Maintaining clean commit history.
      4.	Reusable Code Snippets
            -- Helper functions for text preprocessing.
            -- Sample configuration files for experiments.
      
Alignment with Other Branches:
  - feature/rag-llm → Focused on Retrieval-Augmented Generation (RAG) and LLM evaluation frameworks.
  - feature/agents → Focused on design, evaluation, and orchestration of AI agents.
    
The feature/basic-experiments branch supports these advanced branches by acting as the learning ground and reference point for best practices.

**Next Steps**

  - Add Python scripts for simple experiments.
  - Introduce automated testing with pytest.
  - Document results from small-scale evaluations.
  - Maintain incremental improvements through clear commits.
