# ai-quality-engineering-sandbox
“Sandbox for AI Quality Engineering experiments: testing RAG pipelines, agents, and automation strategies.”
A sandbox repository to explore and practice **Quality Engineering for AI systems**.  
This space is dedicated to experimenting with:  

-  **Testing RAG pipelines** – retrieval accuracy, grounding, hallucination control  
-  **Evaluating AI Agents** – reliability, tool orchestration, decision-making  
-  **Automation Frameworks** – scalable, repeatable QA strategies for LLMs  
-  **Experimental Prototypes** – quick POCs to validate ideas 

Repository Structure (Planned) 
ai-quality-engineering-sandbox/
│
├── rag-experiments/ # Retrieval-Augmented Generation evaluation
├── agents-experiments/ # AI Agent behavior testing
├── automation-snippets/ # Small automation utilities
└── docs/ # Notes, research, design documents

Goals of this Sandbox  
- Practice designing **test strategies** for AI systems  
- Build **POC frameworks** that simulate real-world QA scenarios  
- Gain hands-on Git & GitHub experience while working with AI projects  
- Showcase a **professional approach** to AI Quality Engineering

Tech Stack (Planned)  
- **Python** – main scripting language  
- **LangChain / LlamaIndex** – for RAG/Agent experiments  
- **DeepEval / Ragas** – for LLM evaluation  
- **Pytest** – automation & reproducibility

  Author  
**Avinash Muktevi**  
- QA Engineer | 11+ years in Test Automation & Strategy  
- Exploring **AI Quality Engineering** | RAG | LLMs | Agents
