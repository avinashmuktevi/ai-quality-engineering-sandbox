**AI Quality Engineering Sandbox**
**Overview**
- This repository serves as a personal sandbox for exploring the intersection of Quality Engineering (QE) and Artificial Intelligence (AI). 
- It is structured to showcase practical experiments, design evaluations, and learning journeys in emerging AI-driven testing approaches such as RAG pipelines, LLM evaluation, and          intelligent agent testing.
- The goal of this repository is to demonstrate applied skills, organized thinking, and professional practices aligned with modern software quality assurance and AI system evaluation.
________________________________________
**Repository Structure**
This project is organized into dedicated branches, each focusing on a key area of experimentation:
- feature/basic-experiments
  - Contains fundamental exercises and small-scale experiments to build comfort with Git, Python, and AI testing workflows.
- feature/rag-llm
  - Focuses on Retrieval-Augmented Generation (RAG) pipelines, prompt engineering, and evaluation frameworks for Large Language Models (LLMs).
- feature/agents
  - Explores intelligent agent design, testing strategies, and quality considerations for autonomous AI-driven systems.
**Purpose**
This repository is designed to:
 - Illustrate hands-on practice with modern AI testing approaches.
 - Provide a structured showcase for recruiters and collaborators.
 - Serve as a learning log that demonstrates steady growth in AI Quality Engineering.
**Future Enhancements**
 - Add reusable testing utilities.
 - Document end-to-end RAG evaluation workflows.
 - Explore benchmark comparisons for agent reliability.
