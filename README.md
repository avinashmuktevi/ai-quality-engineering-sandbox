**AI Quality Engineering Sandbox**

**Overview**
- This repository serves as a personal sandbox for exploring the intersection of Quality Engineering (QE) and Artificial Intelligence (AI). 
- It is structured to showcase practical experiments, design evaluations, and learning journeys in emerging AI-driven testing approaches such as RAG pipelines, LLM evaluation, and          intelligent agent testing.
- The goal of this repository is to demonstrate applied skills, organized thinking, and professional practices aligned with modern software quality assurance and AI system evaluation.
________________________________________
**Repository Structure**
This project is organized into dedicated branches, each focusing on a key area of experimentation:
- <u>feature/basic-experiments</u>:
  - Contains fundamental exercises and small-scale experiments to build comfort with Git, Python, and AI testing workflows.
- <u>feature/rag-llm</u>:
  - Focuses on Retrieval-Augmented Generation (RAG) pipelines, prompt engineering, and evaluation frameworks for Large Language Models (LLMs).
- <u>feature/agents</u>:
  - Explores intelligent agent design, testing strategies, and quality considerations for autonomous AI-driven systems.


**Future Enhancements**
 - Add reusable testing utilities.
 - Document end-to-end RAG evaluation workflows.
 - Explore benchmark comparisons for agent reliability.

